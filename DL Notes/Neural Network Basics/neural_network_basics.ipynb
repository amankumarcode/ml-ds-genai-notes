{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Neural Network Basics\n",
        "\n",
        "A gentle, practical introduction to deep learning fundamentals. This notebook explains key ideas in simple terms: what ML vs DL means, how neural networks are built, what MLPs are, common activation and loss functions, and how gradient descent and backpropagation train a network.\n",
        "\n",
        "- **Audience**: Beginners who want an intuitive overview\n",
        "- **Goal**: Understand core building blocks and how they fit together \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## What is Machine Learning (ML) vs Deep Learning (DL)?\n",
        "\n",
        "- **Machine Learning (ML)**: Teach computers to make predictions/decisions from data. You provide features (inputs) and true labels/targets; the algorithm learns patterns.\n",
        "- **Deep Learning (DL)**: A subset of ML that uses multi-layer neural networks to automatically learn useful representations (features) from raw data (images, audio, text), often at large scale.\n",
        "\n",
        "Key differences in simple terms:\n",
        "- **Feature engineering**: ML often needs hand-crafted features; DL learns features end-to-end from data.\n",
        "- **Data scale**: DL shines with lots of labeled data and compute.\n",
        "- **Model depth**: DL stacks many layers (hence \"deep\").\n",
        "\n",
        "When to use DL:\n",
        "- You have complex inputs (images, audio, text) and enough data.\n",
        "- You want to replace manual feature engineering with learned representations. \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Neural Network Structure (High-Level)\n",
        "\n",
        "A neural network is a stack of layers that transform inputs into outputs. Each layer applies a simple operation and passes its result to the next layer.\n",
        "\n",
        "- **Neurons**: Small units that compute a weighted sum of inputs plus a bias, then pass it through an activation function.\n",
        "- **Layers**: Collections of neurons. Common types: input, hidden, output.\n",
        "- **Weights and biases**: Parameters the model learns to minimize error.\n",
        "- **Activation function**: Non-linearity that lets networks learn complex patterns.\n",
        "\n",
        "Basic forward pass idea:\n",
        "1. Start with input features x.\n",
        "2. Multiply by weights and add bias: z = W x + b.\n",
        "3. Apply activation: a = activation(z).\n",
        "4. Repeat across layers until you get predictions. \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Multilayer Perceptrons (MLPs)\n",
        "\n",
        "An MLP is a classic feedforward neural network:\n",
        "- Layers are arranged in a sequence: input → one or more hidden layers → output.\n",
        "- Each hidden layer fully connects to the next (every neuron connects to all neurons in the next layer).\n",
        "- Suitable for tabular data and as building blocks in many models.\n",
        "\n",
        "Key ideas:\n",
        "- **Capacity vs overfitting**: More layers/neurons can model more complex patterns but may overfit small datasets.\n",
        "- **Regularization**: Techniques like dropout, weight decay, early stopping reduce overfitting.\n",
        "- **Initialization & normalization**: Good weight init and normalization (e.g., batch norm) help stable training. \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Activation Functions (Intuition + Common Choices)\n",
        "\n",
        "Why we need them:\n",
        "- Without activations, a stack of linear layers collapses to a single linear transform. Activations add non-linearity so networks can model complex patterns.\n",
        "\n",
        "Common functions:\n",
        "- **ReLU (Rectified Linear Unit)**: `ReLU(z) = max(0, z)`\n",
        "  - Simple, fast, works well in many models.\n",
        "  - Variants: LeakyReLU, GELU.\n",
        "- **Sigmoid**: Squashes values to (0, 1). Often used for binary classification output.\n",
        "- **Tanh**: Squashes to (-1, 1). Zero-centered, but can saturate.\n",
        "- **Softmax**: Turns a vector into probabilities that sum to 1. Used for multi-class outputs.\n",
        "\n",
        "Practical tips:\n",
        "- Start with ReLU (or GELU) in hidden layers.\n",
        "- Use sigmoid for binary output, softmax for multi-class output. \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Loss (Cost) Functions\n",
        "\n",
        "The loss tells us how wrong the model is. Training means adjusting weights to reduce this loss.\n",
        "\n",
        "Common choices:\n",
        "- **Mean Squared Error (MSE)**: For regression (predicting numbers). Penalizes squared difference between prediction and target.\n",
        "- **Binary Cross-Entropy (Log Loss)**: For binary classification. Compares predicted probability vs true label (0/1).\n",
        "- **Categorical Cross-Entropy**: For multi-class classification. Used with softmax outputs.\n",
        "\n",
        "Notes:\n",
        "- Loss choice should match the task and output activation.\n",
        "- Add regularization terms (e.g., L2) to discourage overly large weights. \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Gradient Descent (How We Learn)\n",
        "\n",
        "Goal: Find weights that minimize the loss.\n",
        "\n",
        "Simple idea:\n",
        "1. Pick initial weights (random).\n",
        "2. Compute the loss on your data.\n",
        "3. Compute the gradient: how much each weight affects the loss.\n",
        "4. Move weights a little in the direction that reduces the loss.\n",
        "5. Repeat until the loss stops improving.\n",
        "\n",
        "Update rule (conceptually):\n",
        "- New weights = old weights − learning_rate × gradient\n",
        "\n",
        "Variants:\n",
        "- **SGD**: Uses small batches of data at a time.\n",
        "- **Momentum**: Smooths updates to move faster in the right direction.\n",
        "- **Adam**: Adapts learning rates per-parameter; a strong default in practice. \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Backpropagation (How We Compute Gradients)\n",
        "\n",
        "Backpropagation is a fast way to compute all gradients using the chain rule from calculus.\n",
        "\n",
        "Intuition:\n",
        "- Do a forward pass to compute predictions and loss.\n",
        "- Work backwards layer by layer, figuring out how much each weight contributed to the error.\n",
        "- Use these gradients to update weights (via gradient descent or Adam).\n",
        "\n",
        "Key points:\n",
        "- Efficient: Reuses intermediate results from the forward pass.\n",
        "- Exact (for the given loss and activations), up to numerical precision.\n",
        "- Most DL frameworks (PyTorch, TensorFlow) do this automatically (autograd). \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## A Tiny Worked Example (Conceptual)\n",
        "\n",
        "Imagine a tiny MLP: input (2 features) → hidden (2 neurons, ReLU) → output (1 neuron, sigmoid) for binary classification.\n",
        "\n",
        "Steps:\n",
        "1. Forward: compute z and activations for hidden and output.\n",
        "2. Loss: binary cross-entropy comparing predicted probability vs true label.\n",
        "3. Backward: compute gradients for output weights/bias, then hidden, using chain rule.\n",
        "4. Update: apply gradient descent/Adam with a learning rate.\n",
        "\n",
        "Even though this sounds involved, libraries automate it. You mainly define the model, choose the loss and optimizer, and train. \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Quick Recap\n",
        "\n",
        "- **ML vs DL**: DL uses deep neural networks to learn features directly from data.\n",
        "- **Network structure**: Layers of neurons with weights, biases, and activations.\n",
        "- **MLP**: A sequence of fully connected layers; a basic, versatile architecture.\n",
        "- **Activations**: Add non-linearity; ReLU/GELU are common in hidden layers.\n",
        "- **Loss**: Measures prediction error; choose based on the task.\n",
        "- **Gradient Descent**: Procedure to minimize loss by updating weights.\n",
        "- **Backpropagation**: Efficient method to compute gradients.\n",
        "\n",
        "Next steps: Try building a tiny MLP in your favorite framework (PyTorch or TensorFlow) on a small dataset. \n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
