{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Neural Network Basics\n",
        "\n",
        "A gentle, practical introduction to deep learning fundamentals. This notebook explains key ideas in simple terms: what ML vs DL means, how neural networks are built, what MLPs are, common activation and loss functions, and how gradient descent and backpropagation train a network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogReg acc: 0.8444444444444444\n",
            "MLP acc: 0.9388888888888889\n"
          ]
        }
      ],
      "source": [
        "# Simple contrast: linear model vs small MLP in scikit-learn\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "X, y = make_moons(n_samples=600, noise=0.2, random_state=45)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=45)\n",
        "\n",
        "lr = LogisticRegression().fit(X_train, y_train)\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(16, 16), max_iter=500, random_state=45).fit(X_train, y_train)\n",
        "\n",
        "print(\"LogReg acc:\", accuracy_score(y_test, lr.predict(X_test)))\n",
        "print(\"MLP acc:\", accuracy_score(y_test, mlp.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## What is Machine Learning (ML) vs Deep Learning (DL)?\n",
        "\n",
        "- **Machine Learning (ML)**: Teach computers to make predictions/decisions from data. You provide features (inputs) and true labels/targets; the algorithm learns patterns.\n",
        "- **Deep Learning (DL)**: A subset of ML that uses multi-layer neural networks to automatically learn useful representations (features) from raw data (images, audio, text), often at large scale.\n",
        "\n",
        "Key differences in simple terms:\n",
        "- **Feature engineering**: ML often needs hand-crafted features; DL learns features end-to-end from data.\n",
        "- **Data scale**: DL shines with lots of labeled data and compute.\n",
        "- **Model depth**: DL stacks many layers (hence \"deep\").\n",
        "\n",
        "When to use DL:\n",
        "- You have complex inputs (images, audio, text) and enough data.\n",
        "- You want to replace manual feature engineering with learned representations. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z1: [ 0.1  -0.11]\n",
            "h1: [0.1 0. ]\n",
            "z2 (logit): [0.15]\n"
          ]
        }
      ],
      "source": [
        "# Minimal forward pass through a 2-layer perceptron\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([0.2, -0.4])\n",
        "W1 = np.array([[0.3, -0.1],\n",
        "               [0.25, 0.4]])\n",
        "b1 = np.array([0.0, 0.0])\n",
        "W2 = np.array([[0.5],\n",
        "               [-0.3]])\n",
        "b2 = np.array([0.1])\n",
        "\n",
        "relu = lambda z: np.maximum(0, z)\n",
        "\n",
        "z1 = W1 @ x + b1\n",
        "h1 = relu(z1)\n",
        "z2 = W2.T @ h1 + b2\n",
        "print(\"z1:\", z1)\n",
        "print(\"h1:\", h1)\n",
        "print(\"z2 (logit):\", z2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Neural Network Structure (High-Level)\n",
        "\n",
        "A neural network is a stack of layers that transform inputs into outputs. Each layer applies a simple operation and passes its result to the next layer.\n",
        "\n",
        "- **Neurons**: Small units that compute a weighted sum of inputs plus a bias, then pass it through an activation function.\n",
        "- **Layers**: Collections of neurons. Common types: input, hidden, output.\n",
        "- **Weights and biases**: Parameters the model learns to minimize error.\n",
        "- **Activation function**: Non-linearity that lets networks learn complex patterns.\n",
        "\n",
        "Basic forward pass idea:\n",
        "1. Start with input features x.\n",
        "2. Multiply by weights and add bias: z = W x + b.\n",
        "3. Apply activation: a = activation(z).\n",
        "4. Repeat across layers until you get predictions. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manual feature acc: 0.63\n",
            "MLP acc: 0.82\n"
          ]
        }
      ],
      "source": [
        "# Tiny example contrasting ML vs DL (feature engineering vs end-to-end)\n",
        "# Here, we simulate a hand-crafted feature and a small MLP on raw features\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1)\n",
        "X = np.random.randn(100, 2)\n",
        "y = (X[:, 0] * 0.7 + (X[:, 1] > 0).astype(float) + 0.2*np.random.randn(100) > 0).astype(int)\n",
        "\n",
        "# \"Manual feature\": thresholded second feature\n",
        "feat_manual = (X[:, 1] > 0).astype(float)\n",
        "acc_manual = (feat_manual == y).mean()\n",
        "\n",
        "# Simple 2-4-1 MLP trained by a few steps of SGD (very rough)\n",
        "W1 = np.random.randn(2, 4) * 0.5\n",
        "b1 = np.zeros(4)\n",
        "W2 = np.random.randn(4, 1) * 0.5\n",
        "b2 = np.zeros(1)\n",
        "\n",
        "relu = lambda z: np.maximum(0, z)\n",
        "sigmoid = lambda z: 1/(1+np.exp(-z))\n",
        "\n",
        "lr = 0.1\n",
        "for _ in range(50):\n",
        "    # batch gradient (not optimized)\n",
        "    Z1 = X @ W1 + b1\n",
        "    H1 = relu(Z1)\n",
        "    P = sigmoid(H1 @ W2 + b2)[:, 0]\n",
        "    # gradients\n",
        "    dZ2 = P - y\n",
        "    dW2 = H1.T @ dZ2[:, None] / len(X)\n",
        "    db2 = dZ2.mean()\n",
        "    dH1 = dZ2[:, None] @ W2.T\n",
        "    dZ1 = dH1 * (Z1 > 0)\n",
        "    dW1 = X.T @ dZ1 / len(X)\n",
        "    db1 = dZ1.mean(axis=0)\n",
        "    # update\n",
        "    W2 -= lr*dW2\n",
        "    b2 -= lr*db2\n",
        "    W1 -= lr*dW1\n",
        "    b1 -= lr*db1\n",
        "\n",
        "# evaluate\n",
        "P = sigmoid(relu(X @ W1 + b1) @ W2 + b2)[:, 0]\n",
        "pred = (P > 0.5).astype(int)\n",
        "acc_mlp = (pred == y).mean()\n",
        "\n",
        "print(f\"Manual feature acc: {acc_manual:.2f}\\nMLP acc: {acc_mlp:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Multilayer Perceptrons (MLPs)\n",
        "\n",
        "An MLP is a classic feedforward neural network:\n",
        "- Layers are arranged in a sequence: input → one or more hidden layers → output.\n",
        "- Each hidden layer fully connects to the next (every neuron connects to all neurons in the next layer).\n",
        "- Suitable for tabular data and as building blocks in many models.\n",
        "\n",
        "Key ideas:\n",
        "- **Capacity vs overfitting**: More layers/neurons can model more complex patterns but may overfit small datasets.\n",
        "- **Regularization**: Techniques like dropout, weight decay, early stopping reduce overfitting.\n",
        "- **Initialization & normalization**: Good weight init and normalization (e.g., batch norm) help stable training. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z: [0.096 0.164 0.058 0.077]\n",
            "h (ReLU): [0.096 0.164 0.058 0.077]\n"
          ]
        }
      ],
      "source": [
        "# Simple MLP layer forward pass\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "x = np.array([1.0, -1.0, 0.5])   # 3 input features\n",
        "W = np.random.randn(4, 3) * 0.1   # 4 neurons x 3 inputs\n",
        "b = np.zeros(4)\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "z = W @ x + b\n",
        "h = relu(z)\n",
        "print(\"z:\", np.round(z, 3))\n",
        "print(\"h (ReLU):\", np.round(h, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Activation Functions (Intuition + Common Choices)\n",
        "\n",
        "Why we need them:\n",
        "- Without activations, a stack of linear layers collapses to a single linear transform. Activations add non-linearity so networks can model complex patterns.\n",
        "\n",
        "Common functions:\n",
        "- **ReLU (Rectified Linear Unit)**: `ReLU(z) = max(0, z)`\n",
        "  - Simple, fast, works well in many models.\n",
        "  - Variants: LeakyReLU, GELU.\n",
        "- **Sigmoid**: Squashes values to (0, 1). Often used for binary classification output.\n",
        "- **Tanh**: Squashes to (-1, 1). Zero-centered, but can saturate.\n",
        "- **Softmax**: Turns a vector into probabilities that sum to 1. Used for multi-class outputs.\n",
        "\n",
        "Practical tips:\n",
        "- Start with ReLU (or GELU) in hidden layers.\n",
        "- Use sigmoid for binary output, softmax for multi-class output. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z: [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\n",
            "ReLU: [0. 0. 0. 0. 0. 0. 1. 2. 3. 4. 5.]\n",
            "Sigmoid: [0.007 0.018 0.047 0.119 0.269 0.5   0.731 0.881 0.953 0.982 0.993]\n",
            "Tanh: [-1.    -0.999 -0.995 -0.964 -0.762  0.     0.762  0.964  0.995  0.999\n",
            "  1.   ]\n"
          ]
        }
      ],
      "source": [
        "# Activation functions in NumPy\n",
        "import numpy as np\n",
        "\n",
        "z = np.linspace(-5, 5, 11)\n",
        "relu = np.maximum(0, z)\n",
        "sigmoid = 1/(1+np.exp(-z))\n",
        "tanh = np.tanh(z)\n",
        "\n",
        "print(\"z:\", z)\n",
        "print(\"ReLU:\", relu)\n",
        "print(\"Sigmoid:\", np.round(sigmoid, 3))\n",
        "print(\"Tanh:\", np.round(tanh, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Loss (Cost) Functions\n",
        "\n",
        "The loss tells us how wrong the model is. Training means adjusting weights to reduce this loss.\n",
        "\n",
        "Common choices:\n",
        "- **Mean Squared Error (MSE)**: For regression (predicting numbers). Penalizes squared difference between prediction and target.\n",
        "- **Binary Cross-Entropy (Log Loss)**: For binary classification. Compares predicted probability vs true label (0/1).\n",
        "- **Categorical Cross-Entropy**: For multi-class classification. Used with softmax outputs.\n",
        "\n",
        "Notes:\n",
        "- Loss choice should match the task and output activation.\n",
        "- Add regularization terms (e.g., L2) to discourage overly large weights. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 0.020000000000000028\n",
            "BCE( p=0.9, y=1 ): 0.10536051565782628\n",
            "BCE( p=0.1, y=1 ): 2.3025850929940455\n"
          ]
        }
      ],
      "source": [
        "# Simple loss function examples\n",
        "import numpy as np\n",
        "\n",
        "# Regression with MSE\n",
        "y_true = np.array([2.5, 0.0, 2.1])\n",
        "y_pred = np.array([2.7, -0.1, 2.0])\n",
        "mse = np.mean((y_pred - y_true)**2)\n",
        "print(\"MSE:\", mse)\n",
        "\n",
        "# Binary classification with BCE\n",
        "def bce(p, y):\n",
        "    p = np.clip(p, 1e-9, 1-1e-9)\n",
        "    return -(y*np.log(p) + (1-y)*np.log(1-p))\n",
        "\n",
        "print(\"BCE( p=0.9, y=1 ):\", bce(0.9, 1))\n",
        "print(\"BCE( p=0.1, y=1 ):\", bce(0.1, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Gradient Descent (How We Learn)\n",
        "\n",
        "Goal: Find weights that minimize the loss.\n",
        "\n",
        "Simple idea:\n",
        "1. Pick initial weights (random).\n",
        "2. Compute the loss on your data.\n",
        "3. Compute the gradient: how much each weight affects the loss.\n",
        "4. Move weights a little in the direction that reduces the loss.\n",
        "5. Repeat until the loss stops improving.\n",
        "\n",
        "Update rule (conceptually):\n",
        "- New weights = old weights − learning_rate × gradient\n",
        "\n",
        "Variants:\n",
        "- **SGD**: Uses small batches of data at a time.\n",
        "- **Momentum**: Smooths updates to move faster in the right direction.\n",
        "- **Adam**: Adapts learning rates per-parameter; a strong default in practice. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step=00 w=1.8000 f(w)=1.4400\n",
            "step=01 w=2.5200 f(w)=0.2304\n",
            "step=02 w=2.8080 f(w)=0.0369\n",
            "step=03 w=2.9232 f(w)=0.0059\n",
            "step=04 w=2.9693 f(w)=0.0009\n",
            "step=05 w=2.9877 f(w)=0.0002\n",
            "step=06 w=2.9951 f(w)=0.0000\n",
            "step=07 w=2.9980 f(w)=0.0000\n",
            "step=08 w=2.9992 f(w)=0.0000\n",
            "step=09 w=2.9997 f(w)=0.0000\n"
          ]
        }
      ],
      "source": [
        "# Gradient descent visualization for a simple quadratic\n",
        "import numpy as np\n",
        "\n",
        "f = lambda w: (w - 3)**2\n",
        "fprime = lambda w: 2*(w - 3)\n",
        "\n",
        "w = 0.0\n",
        "lr = 0.3\n",
        "for step in range(10):\n",
        "    grad = fprime(w)\n",
        "    w = w - lr * grad\n",
        "    print(f\"step={step:02d} w={w:.4f} f(w)={f(w):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Backpropagation (How We Compute Gradients)\n",
        "\n",
        "Backpropagation is a fast way to compute all gradients using the chain rule from calculus.\n",
        "\n",
        "Intuition:\n",
        "- Do a forward pass to compute predictions and loss.\n",
        "- Work backwards layer by layer, figuring out how much each weight contributed to the error.\n",
        "- Use these gradients to update weights (via gradient descent or Adam).\n",
        "\n",
        "Key points:\n",
        "- Efficient: Reuses intermediate results from the forward pass.\n",
        "- Exact (for the given loss and activations), up to numerical precision.\n",
        "- Most DL frameworks (PyTorch, TensorFlow) do this automatically (autograd). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backprop demonstration with autograd (PyTorch)\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "x = torch.tensor([[0.5, -1.0]], dtype=torch.float32)  # (1,2)\n",
        "y = torch.tensor([[1.0]], dtype=torch.float32)        # (1,1)\n",
        "\n",
        "W1 = torch.randn(2, 2, requires_grad=True) * 0.5\n",
        "b1 = torch.zeros(2, requires_grad=True)\n",
        "W2 = torch.randn(2, 1, requires_grad=True) * 0.5\n",
        "b2 = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "h1 = torch.relu(x @ W1 + b1)\n",
        "p = torch.sigmoid(h1 @ W2 + b2)  # (1,1)\n",
        "\n",
        "loss = torch.nn.functional.binary_cross_entropy(p, y)\n",
        "loss.backward()  # compute gradients via backprop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Quick Recap\n",
        "\n",
        "- **ML vs DL**: DL uses deep neural networks to learn features directly from data.\n",
        "- **Network structure**: Layers of neurons with weights, biases, and activations.\n",
        "- **MLP**: A sequence of fully connected layers; a basic, versatile architecture.\n",
        "- **Activations**: Add non-linearity; ReLU/GELU are common in hidden layers.\n",
        "- **Loss**: Measures prediction error; choose based on the task.\n",
        "- **Gradient Descent**: Procedure to minimize loss by updating weights.\n",
        "- **Backpropagation**: Efficient method to compute gradients.\n",
        "\n",
        "Next steps: Try building a tiny MLP in your favorite framework (PyTorch or TensorFlow) on a small dataset. \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
